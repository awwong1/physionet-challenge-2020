{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration\n",
    "\n",
    "This jupyter notebook is intended to be used for validating potential new neural network architecture designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from datasets import PhysioNet2020Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PhysioNet2020Dataset(\n",
    "    \"Training_WFDB\",\n",
    "    max_seq_len=6000,\n",
    "    records=(\"A0001\", \"A0002\", \"A0003\", \"A0004\"), # (7500, 5000, 5000, 5974)\n",
    "    proc=0,\n",
    "    ensure_equal_len=False,\n",
    "    derive_fft=True\n",
    ")\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    collate_fn=PhysioNet2020Dataset.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=12,\n",
    "        num_classes=9,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        hidden_size=200,\n",
    "        bidirectional=True\n",
    "    ):\n",
    "        super(ExplorationModel, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm_sig = nn.LSTM(\n",
    "            input_size = in_channels,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            dropout = dropout,\n",
    "            bidirectional = bidirectional\n",
    "        )\n",
    "        \n",
    "        self.lstm_fft = nn.LSTM(\n",
    "            input_size = in_channels,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            dropout = dropout,\n",
    "            bidirectional = bidirectional\n",
    "        )\n",
    "        \n",
    "        lstm_hidden_size = hidden_size * 2\n",
    "        if bidirectional:\n",
    "            lstm_hidden_size *= 2\n",
    "\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.BatchNorm1d(lstm_hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "            nn.Linear(lstm_hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        sig = batch[\"signal\"]\n",
    "        fft = batch[\"fft\"]\n",
    "        sig_lens = batch[\"len\"]\n",
    "\n",
    "        lstm_sig_in = pack_padded_sequence(sig, sig_lens, enforce_sorted=False)\n",
    "        _, (sig_hidden, _) = self.lstm_sig(lstm_sig_in)\n",
    "        # out, lens = pad_packed_sequence(packed_out)\n",
    "\n",
    "        lstm_fft_in = pack_padded_sequence(fft, tuple(l//2 for l in sig_lens), enforce_sorted=False)\n",
    "        _, (fft_hidden, _) = self.lstm_sig(lstm_fft_in)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # concat the forward and backward hidden states\n",
    "            sig_hidden  = torch.cat((sig_hidden[-2,:,:], sig_hidden[-1,:,:]), dim=1)\n",
    "            fft_hidden = torch.cat((fft_hidden[-2,:,:], fft_hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            sig_hidden = sig_hidden[-1,:,:]\n",
    "            fft_hidden = fft_hidden[-1,:,:]\n",
    "    \n",
    "        hidden = torch.cat((sig_hidden, fft_hidden), dim=1)\n",
    "        \n",
    "        out = self.classify(hidden)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 9])\n",
      "torch.Size([5, 9])\n"
     ]
    }
   ],
   "source": [
    "m = ExplorationModel()\n",
    "batch = next(iter(dl))\n",
    "out = m(batch)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "m = ExplorationModel(bidirectional=False)\n",
    "batch = next(iter(dl))\n",
    "out = m(batch)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
